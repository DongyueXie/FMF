\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage[margin=1.5in]{geometry}
\usepackage{amsmath,amssymb}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{graphicx}

\title{Empirical Bayes Multiscale Poisson Matrix Factorization}
\author{Dongyue Xie }
\date{Oct 2019}

\begin{document}

\maketitle

\section{Empirical Bayes Multiscale Poisson Smoothing}

Suppose we observe a Poisson sequence $X\in R^n$, $n=2^S$ and $X_i\sim Poisson(\lambda_i)$. A Haar wavelet like decomposition of $X$ is defined as follows. Let $s = 0,1,...,S-1$ denote scale and $l = 0,1,..., 2^s-1$ denote location. $X$ follows a recursive dyadic partition(RDP):
\[X_{S,l} = X_{kl},\]
\[X_{s,l} = X_{s+1,2l}+X_{s+1,2l+1}.\]

Similarly, RDP also holds for $\lambda$.

Define 
\[R_{s,l} = \frac{\lambda_{s+1,2l}}{\lambda_{s,l}},\]
then 
\[X_{s+1,2l}|X_{s,l},R_{s,l}\sim Binomial(X_{s,l},R_{s,l}).\]

This multiscale decomposition reparameterizes $\lambda$ to $\mu:=\lambda_{0,0}$ and $R:=\{R\}_{s,l}$

Following this decomposition, the likelihood of $X$ is 
\begin{equation}
    p(X|\lambda) = \prod_{i=0}^{n-1} p(X_i|\lambda_i) = p(X_{0,0}|\mu)\times \prod_{s=0}^{S-1}\prod_{l=0}^{2^s-1} p(X_{s+1,2l}|X_{s,l},R_{s,l}).
\end{equation}

Further we put priors on $\lambda_{0,0}$ and $R_{s,l}$: $\mu\sim g_\mu(\cdot)$ and $R_{s,l}\sim g_{R_s}(\cdot)$.

An empirical procedure first estimates $g_\mu(\cdot)$ and $g_{R_s}(\cdot)$ by maximizing the marginal likelihood, then obtains posterior $p(R,\mu|X,\hat{g})$.

\begin{equation}
    \begin{split}
        \log p(X|g_\mu,g_R) & = \E_{q_Rq_\mu}\log p(X|g_\mu,g_R)
        \\ & = \E_{q_Rq_\mu}\log \frac{p(X,\mu,R|g_\mu,g_R)}{p(R,\mu|X,g_\mu,g_R)}
        \\ & = \E_{q_Rq_\mu}(\log\frac{p(X,\mu,R|g_\mu,g_R)}{q_Rq_\mu}+\log\frac{q_Rq_\mu}{p(R,\mu|X,g_\mu,g_R)})
        \\ & = \E_{q_Rq_\mu}\log p(X|R,\mu) + \E_{q_Rq_\mu}\log \frac{g_\mu g_R}{q_Rq_\mu} + KL(q_Rq_\mu||p(R,\mu|g_\mu,g_R))
        \\ & = \mathcal{F}(q_R,q_\mu,g_\mu,g_R; X) + KL(q_Rq_\mu||p(R,\mu|g_\mu,g_R))
        \\ & \geq \mathcal{F}(q_R,q_\mu,g_\mu,g_R; X).
    \end{split}
\end{equation}

\begin{equation}\label{elboPS}
\begin{split}
    \mathcal{F}(q_R,q_\mu,g_\mu,g_R; X) &= \E_{q_Rq_\mu}(\sum_{s=0}^{S-1}\sum_{l=0}^{2^s-1}\log p(X_{s+1,2l}|X_{s,l},R_{s,l})+\log p(X_{0,0}|\mu,c)) 
    \\ & + \E_{q_R}\log\frac{g_R}{q_R} + \E_{q_\mu}\log\frac{g_\mu}{q_\mu}.
\end{split}
\end{equation}


\section{Rank-1 EB Multiscale Poisson Matrix Factorization}

Let $Z_{ij}\sim Poisson(l_if_j)$, $i=1,2,...,N$, $j=1,2,...,p$. Priors: $l\sim g_l(\cdot)$. Assume $f\in R^p$ has RDP defined in section 1 and $R_{s,l}\sim g_{R_{s}}(\cdot)$, $\mu\sim g_\mu(\cdot)$. 

Using variational method, the evidence lower bound(ELBO) is 
\begin{equation}\label{elbo}
\begin{split}
    ELBO &= \E_q\log p(Z,l,R,\mu|g_l,g_\mu,g_R) - \E_q\log q(l,R,\mu)
    \\&= \E_q \log p(Z|l,R,\mu) + \E_q\log\frac{g_l}{q_l} + \E_q\log\frac{g_R}{q_R} + \E_q\log\frac{g_\mu}{q_\mu}
\end{split}
\end{equation}

1. Update $q_R, g_R,q_\mu,g_\mu$: given $q_l,g_l$,
\begin{equation}\label{elboR}
\begin{split}
    ELBO(q_R, g_R) &  = \E_q(\sum_{i=1}^N\sum_{s=0}^{S-1}\sum_{l=1}^{2^s-1}\log p(Z_{i,s+1,2l}|Z_{i,s,l},R_{s,l})+\sum_{i=1}^N\log 
    p(Z_{i,0,0}|\mu,l_i) )
    \\ &  + \E_q\log\frac{g_R}{q_R} + \E_q\log\frac{g_\mu}{q_\mu} + Constant
\end{split}
\end{equation}

This first line of ELBO in (\ref{elboR}) is 
\begin{equation}\label{elboR1}
\begin{split}
    \sum_s\sum_l \E_{q_R q_\mu,g_R, g_\mu}( (\Sigma_i Z_i)_{s+1,2l}\log R_{s,l} + (\Sigma_i Z_i)_{s+1,2l+1}\log (1-R_{s,l})  \\
   + \sum_i Z_{i,0,0}\log\mu -\mu\sum_i \E_{q_l}l_i )+ Constant
\end{split}
\end{equation}

Combine (\ref{elbo}), (\ref{elboR}) and (\ref{elboR1}), and compare with (\ref{elboPS}), we can conclude that updating  $q_R, g_R,q_\mu,g_\mu$ in rank-1 case is equivalent to solve Empirical Bayes Multiscale Poisson Smoothing with sequence $\sum_i Z_i$ and an extra scale factor $c = \sum_i \E_{q_l} l_i$.

2. Update $q_l,g_l$: given $q_R, g_R,q_\mu,g_\mu$,

We can write $f_{j} = \mu\prod_{s=0}^{S-1} (R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)}$, where $\epsilon_j(s) = 1$ if the $j$th element of $f$ goes to left children node at scale $s$, and $s(j)$ is the location of $j$th element of $f$ at scale $s$.

Intuitively, this should be equivalent to solve EBPM with sequence $\sum_j Z_j$ and a scale factor $\E_{q_\mu}(\mu)\sum_j \prod_s \E_{q_{R_s}}((R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)})$. The derivations below show the equivalence.

ELBO for rank-1 case:

\begin{equation}\label{elbol}
\begin{split}
ELBO(q_l,g_l) & = \E_{q_R q_\mu q_l} \sum_{i}\sum_j(Z_{ij}\log(l_i\mu\prod_s (R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)})
\\ & - l_i \mu \prod_s (R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)} ) + \E_q\log\frac{g_l}{q_l} + C_1
\\ & = \E_{q_l}(\sum_i (\sum_j Z_{ij}\log l_i - l_i \E_{q_R q_\mu}\sum_j(\mu\prod_{s=0}^{S-1} (R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)})) + \E_q\log\frac{g_l}{q_l} + C_2.
\end{split}
\end{equation}

Solve Empirical Bayes Poisson mean given sequence $\sum_j Z_j$ and a scale factor $f_j = \E_{q_\mu}(\mu)\sum_j \prod_s \E_{q_{R_s}}((R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)})$: 

\begin{equation}\label{ebpml}
\begin{split}
\E_{q_l}\log p(\sum_jZ_j|l,f_j)  & = \E_{q_l} \sum_i (\sum_jZ_j \log l_i - l_if_j)  + C_1
\\&=  \E_{q_l} \sum_i (\sum_jZ_j \log l_i - l_i\times  \E_{q_\mu}(\mu)\sum_j \prod_s \E_{q_{R_s}}((R_{s,s(j)})^{\epsilon_j(s)}(1-R_{s,s(j)})^{1-\epsilon_j(s)})  + C_1
\\
\mathcal{F}(q_l,g_l)  & = \E_{q_l}\log p(\sum_jZ_j|l,f_j)  + \E_q\log\frac{g_l}{q_l} + C_1
\end{split}
\end{equation}

The equivalence holds since $ELBO(q_l,g_l)$ and $\mathcal{F}(q_l,g_l)$ are equivalent.

\section{Rank-K EBMPMF}

\begin{equation}
    \begin{split}
        X = \sum_k Z_k ,
        \\
        Z_k\sim Poisson(l_k f_k^T), l_k\in R^N, f_k\in R^p,
        \\
        f_{kj} = \mu_{k}\prod_{s=0}^{S-1} (R_{k,s,s(j)})^{\epsilon_j(s)}(1-R_{k,s,s(j)})^{1-\epsilon_j(s)},
        \\
        l_k\sim g_{l_k}(\cdot),
        \\
        R_{k,s}\sim g_{R_{k,s}}(\cdot),
        \\
        \mu_k\sim g_{\mu_k}(\cdot),
    \end{split}
\end{equation}

Factorization of joint distribution is 
\begin{equation}
    p(X,Z,L,R,\mu|g) = p(X|Z)p(Z|L,R,\mu)p(R|g_R)p(\mu|g_\mu)p(L|g_L).
\end{equation}

Goal is to estimate $g$ and obtain posterior $p(L,R,\mu|X,\hat{g})$. Two ways to derive variational lower bound: Jensen's Inequality and KL divergence.

Variational lower bound: 
\begin{equation}\label{elbok}
\begin{split}
 \mathcal{F}(q,g) & = \E_q \log p(X,Z,L,R,\mu|g)-\E_q\log q(Z,L,R,\mu)
 \\&=\E_{q}\log p(X,Z,R,L,\mu|g_R,g_\mu,g_L)-\E_{q_L}\log q_L(L) - \E_{q_R}\log q_R(R) - \E_{q_Z}\log q_Z(Z) - \E_q\log q_\mu
        \\&= \E_q\log p(Z|R,L,\mu) + \E_q\log \delta(X-\sum_k Z_k) + \E_q\log\frac{g_R}{q_R} + \E_q\log\frac{g_L}{q_L}+\E_q\log\frac{g_\mu}{q_\mu}-\E_q\log q_Z
        \\&= \sum_k(\E_q\log p(Z_k|R_k,l_k,\mu_k)+\E_q\log\frac{g_{R_k}}{q_{R_k}}+\E_q\log\frac{g_{L_k}}{q_{L_k}}+\E_q\log\frac{g_{\mu_k}}{q_{\mu_k}})+ \E_q\log \delta(X-\sum_k Z_k)-\E_q\log q_Z.
\end{split}
\end{equation}




1. Update $q_Z(Z)$: Take functional derivatives of the lower bound with respect to $q_Z(Z)$, then 
\begin{equation}
    \begin{split}
        q_Z(Z) &\propto \exp (\int \int \log p(X,Z|R,L,\mu,g_R,g_L,g_\mu)q_R q_L q_\mu dR dLd\mu) 
        \\ & \propto \exp(\int\int \log p(Z|X,R,\mu,L,g_R,g_L,g_\mu)q_Rq_L q_\mu dR dLd\mu)
        \\ &\propto \exp (\E_{q_R,q_L,q_\mu}\log p(Z|X,R,L,\mu,g_R,g_L,g_\mu)).
    \end{split}
\end{equation}

This leads to 

\[q_Z(Z_{1:K,i,j})\sim Multinomial(X_{ij},\pi_{1:K,i,j}),\]
where 
\[\pi_{k,i,j} = \frac{\exp(\E\log l_{ik}+\E\log f_{kj})}{\sum_k\exp(\E\log l_{ik}+\E\log f_{kj})},\]
\[f_{kj} =  \mu_k\prod_{s=0}^{S-1} R_{k,s,s(j)}^{\epsilon_j(s)}(1-R_{k,s,s(j)})^{1-\epsilon_j(s)}.\]

The expectation $\E\log f_{kj}$ is then 
\begin{equation}
\E\log f_{kj} = \E_{q_{\mu_k}}\log\mu_k + \sum_{s=0}^{S-1}\E_{R_{k,s}}({\epsilon_j(s)}\log R_{k,s,s(j)}+(1-\epsilon_j(s))\log(1-R_{k,s,s(j)})).
\end{equation}

From the ELBO formula (\ref{elbok}), updating distributions related to $R,\mu,L$ in rank-K problem reduces to solving K rank-1 problems.

2. Update $q_{l_k}(\cdot)$ and $g_{l_k}(\cdot)$:
\begin{equation}
    \begin{split}
        q_L &\propto \exp(\E_{q_R,q_\mu,q_Z}\log p(Z|L,R,\mu)p(L|g_L))
        \\&\propto
        \exp(\sum_k(\E_q\log p(Z_k|L_k,R_k,\mu_k)p(L_k|g_{L_k}))).
    \end{split}
\end{equation}

We update $q_{l_k}$ for each topic $k$. The $q_{l_k}$ and $g_{l_k}$ can be updated by solving the following Empirical Bayes Poisson mean problem:

\begin{equation}
\begin{split}
\sum_j \E_{q_Z}(Z_{k,i,j}) \sim Poisson (l_{ik}\sum_j \E_{q_{R_k},q_{\mu_k}}(f_{kj})),  i=1,2,...,N,
\\
\E_{q_{R_k},q_{\mu_k}}(f_{kj}) = \E_{q_{\mu_k}}\mu_k \times \prod_{s=0}^{S-1}\E_{R_{k,s}}(R_{k,s,s(j)}^{\epsilon_j(s)}(1-R_{k,s,s(j)})^{1-\epsilon_j(s)}),
\\
l_{k}\sim g_{l_k}(\cdot).
\end{split}
\end{equation}

3. Update $q_{R_k}, q_{\mu_k},g_{R_k}, g_{\mu_k}$: 

Solve the following Empirical Bayes Multiscale Poisson Smoothing problem: given a Poisson sequence $\sum_i\E_{q_Z}(Z_{k,i,j}), j=1,2,...,p$ and an extra scale factor $c = \sum_{i}E_{q_{l_k}}l_{ik}$.

\section{Practical Concerns}

1. Empirical Bayes Poisson Smoothing.

We have at least two methods, all of which depend on the multiscale decomposition of Poisson sequence. The first one is the standard BMSM models in Kolaczyk(1999). Another one is in Xing et al(2019).

For $\mu_k$, the simplest way is to set $\hat{\mu}_k = \sum_i\sum_j\E_{q_Z}(Z_{k,i,j}) / \sum_i(l_{ik})$. This retains the property of preservation of energy and results in an estimate relatively unaffected by perturbations in the data at their original scale.

For $R_{k,s}$, one way is to put a mixture of Beta distribution prior separately for each scale \[R_{k,s,l}\sim \sum_h p_{k,s,h}Beta(\alpha_{k,s,h},\alpha_{k,s,h}).\]

To force smoothness in each $f$, we can also put a point mass on $\frac{1}{2}$. Another way is reparameterize $R_{k,s,l}$ to its logit form $\log\frac{R_{k,s,l}}{1-R_{k,s,l}}$, then use normal approximation hence standard ash prior to achieve shrinkage estimate. 

A common practice in wavelet shrinkage estimation is to use 'cycle spinning' or Non-decimated WT to get translation-invariant property. 

To use the mixture of beta distribution, or more generally shrinking the probability directly, we need to  develop Empirical Bayes Binomial probability . 


\section{To do}

Build code based on EBPMF implementations. 

How to deal with many 0's? Is zero-inflated model possible?

Run some simulation examples. 

Potential applications.

Other modifications to the model 


\end{document}